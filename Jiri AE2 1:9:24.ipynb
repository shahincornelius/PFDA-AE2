{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import string\n",
    "\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Random Wikipedia URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random articles to scrape\n",
    "article_count = 150\n",
    "\n",
    "# Loop to iterate over random articles\n",
    "for i in range(article_count):\n",
    "\n",
    " # Send a GET request to the URL\n",
    "    response = requests.get('https://en.wikipedia.org/wiki/Special:Random')\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "\n",
    "    # Nested loop for saving paragraph data from each article to a file\n",
    "    # Extract the text content from the article\n",
    "    text_content = ''\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text_content += paragraph.get_text() + '\\n'\n",
    "\n",
    "        # Write the text content to the file\n",
    "        # Using 'a' too append text from each iteration of the loop\n",
    "        with open('dataset.txt', 'a', encoding='utf-8') as file:\n",
    "            file.write(text_content.strip())\n",
    "\n",
    "    # Adding a delay to the end of the loop so that Wikipedia won't kick me \n",
    "    time.sleep(.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading a list of stopwords\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "# **IMPORTANT** Un-hash the previous line if nltk.download(\"stopwords\") has not previously been run\n",
    "\n",
    "# Defining a set where each element is a stopword to include in the following translation table\n",
    "stops = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to normalize text\n",
    "def convert_utf(text):\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    return text.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a wider function to turn text into training-ready data \n",
    "\n",
    "def clean(data):\n",
    "    # Converting utf-8 characters to normal characters using previously defined function\n",
    "    data = convert_utf(data)\n",
    "    \n",
    "    # Lowercasing the text data\n",
    "    data = data.lower()\n",
    "\n",
    "    # Expanding any contractions\n",
    "    data = contractions.fix(data)\n",
    "    \n",
    "    # Tokenizing the text data by sentence\n",
    "    sentences = nltk.sent_tokenize(data)\n",
    "\n",
    "    # Defining an empty list + for loop to append word-level tokens into\n",
    "    tokenized_data = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Using word_tokenize() to break up the sentence level tokens into word level tokens\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # Defining another empty list to append non-stopword tokens into. This is to keep track of sentences\n",
    "        cleaned_sentences = []\n",
    "        \n",
    "\n",
    "        # Marking each token as not containing punctuation by default\n",
    "        for token in words:\n",
    "            contains_punc = False\n",
    "\n",
    "            # Iterating over each character in the word to check if any characters are punctuation marks\n",
    "            for character in token:\n",
    "                if character in string.punctuation:\n",
    "                    contains_punc = True\n",
    "        \n",
    "                    # Ending the for loop once punctuation is found for optimization\n",
    "                    break\n",
    "            \n",
    "            # Skipping current token if it contains punctuation so as to not append \n",
    "            if contains_punc:\n",
    "                continue\n",
    "\n",
    "            # Referencing previously defined set of stopwords and a string containing all punctuation marks\n",
    "            if token not in stops:\n",
    "                cleaned_sentences.append(token)\n",
    "                # print(token)\n",
    "        \n",
    "        tokenized_data.append(cleaned_sentences)\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the written file\n",
    "\n",
    "with open(\"./dataset.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Cleaning the text with the previously defined function\n",
    "data0 = clean(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty model\n",
    "\n",
    "model = gensim.models.Word2Vec(vector_size=500, min_count=1, sg=0)\n",
    "model.save(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a model with the preprocessed data\n",
    "\n",
    "model.build_vocab(data0, update=False)\n",
    "model.train(data0, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model.\n",
    "model = gensim.models.Word2Vec.load(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vec = model.wv[\"long\"]\n",
    "\n",
    "# opposite_word = model.wv.most_similar(negative=[word_vec], topn=5)\n",
    "# print(opposite_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Additional Data\n",
    "\n",
    "The model is not performing well, so I am adding full books from Project Gutenberg.\n",
    "\n",
    "Note that the link to each book is strange, as the url indicates a page number, but the page delivers error 404 unless the two numbers are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over links to full books on Project Gutenberg\n",
    "for i in range(50):\n",
    "      request = requests.get(f'https://www.gutenberg.org/cache/epub/{i}/pg{i}.txt')\n",
    "      \n",
    "      # Writing the text content to a file\n",
    "      # Using 'a' too append text from each iteration of the loop, so all 100 books are in 1 file\n",
    "      with open('gutenberg_dataset.txt', 'a', encoding='utf-8') as file:\n",
    "                  file.write(request.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the newly written file\n",
    "with open(\"./gutenberg_dataset.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    data1 = file.read()\n",
    "\n",
    "# Cleaning the text with the previously defined function\n",
    "data1 = clean(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-training the model with the new data\n",
    "\n",
    "# Setting update=True so that the new data is added to the model's current vocabulary, rather than overwriting it\n",
    "model.build_vocab(data1, update=True)\n",
    "model.train(data1, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('small', 0.6334516406059265), ('communities', 0.48378485441207886), ('elsewhere', 0.47936201095581055), ('larger', 0.4593963921070099), ('indefinite', 0.39229339361190796)]\n"
     ]
    }
   ],
   "source": [
    "reference_pair = (\"full\", \"empty\")\n",
    "target_word = \"small\"\n",
    "result_vector = model.wv[target_word] - model.wv[reference_pair[0]] + model.wv[reference_pair[1]]\n",
    "opposite_words = model.wv.similar_by_vector(result_vector, topn=5)\n",
    "\n",
    "print(opposite_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
